{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[{"file_id":"1yEViY5SJvqcn6tfP4B3LRJtKstFj5urG","timestamp":1605644263904}]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"eDuBLEXtpvNX"},"source":["from __future__ import absolute_import, division, print_function\n","import tensorflow as tf\n","import numpy as np\n","import os\n","import time\n","from tensorflow.keras.datasets import fashion_mnist"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YaGeaTEYu1_k"},"source":["## Fashion_Minist data processing"]},{"cell_type":"code","metadata":{"id":"I5hZXIfLpvNY"},"source":["# Read in the fashion_MNIST dataset\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","\n","x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n","# Normalize images value from [0, 255] to [0, 1]\n","x_train, x_test = x_train / 255., x_test / 255.\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZBNezXz2yyls","executionInfo":{"status":"ok","timestamp":1605842059387,"user_tz":360,"elapsed":305,"user":{"displayName":"Hongyu Guo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgexSoQ9_QdGEClNISOan7UQbqY6oUUgE9thCgEug=s64","userId":"07582815621439998065"}},"outputId":"fe53ce98-09dd-45d6-9fea-5ed6f1b5f332"},"source":["print(' Size of x_train: ',x_train.shape, '\\n', 'Size of x_test: ',  x_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" Size of x_train:  (60000, 28, 28) \n"," Size of x_test:  (10000, 28, 28)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mjz1SU11uutc"},"source":["## Define super parameters"]},{"cell_type":"code","metadata":{"id":"f_LqJRN9pvNY"},"source":["num_classes = 10 # total classes of Fashion-MNIST dataset\n","learning_rate = 0.001\n","epoch = 5\n","batch_size = 128\n","training_steps = int(x_train.shape[0]/batch_size) * epoch\n","display_step = int(x_train.shape[0]/batch_size)/2\n","# Network parameters.\n","conv1_filters = 6 # number of filters for 1st conv layer.\n","conv2_filters = 16 # number of filters for 2nd conv layer.\n","fc1_units = 120 # number of neurons for 1st fully-connected layer.\n","fc2_units = 84  # number of neurons for 2st fully-connected layer."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YURSezAzodpp"},"source":["# Shuffle and batch data.\n","train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uw4yRlBbu9it"},"source":["## Define convolution and pooling function"]},{"cell_type":"code","metadata":{"id":"twEiD6lCpvNY"},"source":["def conv2d(x, W, b, strides=1):\n","    # Conv2D wrapper, with bias and relu activation.\n","    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='VALID')\n","    x = tf.nn.bias_add(x, b)\n","    return tf.nn.relu(x)\n","\n","def maxpool2d(x, k=2):\n","    # MaxPool2D wrapper.\n","    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n","\n","# Weights initialization\n","def weights_init():\n","    random_normal = tf.initializers.RandomNormal()\n","    weights = {\n","        # Conv Layer 1: 5x5 conv,  6 filters \n","        'wc1': tf.Variable(random_normal([5, 5, 1, conv1_filters])),\n","        # Conv Layer 2: 5x5 conv, 16 filters.\n","        'wc2': tf.Variable(random_normal([5, 5, conv1_filters, conv2_filters])),\n","        # FC Layer 1: 4*4*64 inputs, 120 units.\n","        'wd1': tf.Variable(random_normal([4*4*16, fc1_units])),\n","        # FC Layer 2: 120 inputs, 84 units.\n","        'wd2': tf.Variable(random_normal([fc1_units, fc2_units])),\n","        # FC Out Layer: 84 inputs, 10 units (total number of classes)\n","        'w_out': tf.Variable(random_normal([fc2_units, num_classes]))\n","    }\n","    biases = {\n","        'bc1': tf.Variable(tf.zeros([conv1_filters])),\n","        'bc2': tf.Variable(tf.zeros([conv2_filters])),\n","        'bd1': tf.Variable(tf.zeros([fc1_units])),\n","        'bd2': tf.Variable(tf.zeros([fc2_units])),\n","        'bout': tf.Variable(tf.zeros([num_classes]))\n","    }\n","\n","    return weights, biases \n","\n","weights, biases = weights_init()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZFuQkklTvG9O"},"source":["# Question 1:  Baseline Model\n","# LeNet-5 style architecture\n"]},{"cell_type":"code","metadata":{"id":"KIRDdzxBpvNZ"},"source":["# Define model \n","def conv_net(x):\n","    # Input shape: [batch_size, 28, 28, 1]. \n","    # A batch of 28x28x1 (grayscale) images.\n","    x = tf.reshape(x, [-1, 28, 28, 1])\n","    # Convolution Layer. Output shape: [batch_size, 24, 24, 6].\n","    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n","    # Max Pooling. Output shape: [batch_size, 12, 12, 6].\n","    conv1 = maxpool2d(conv1, k=2)\n","   \n","    # Convolution Layer. Output shape: [batch_size, 8, 8, 16].\n","    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n","    # Max Pooling. Output shape: [batch_size, 4, 4, 16].\n","    conv2 = maxpool2d(conv2, k=2)\n","\n","    # Reshape conv2 output to fit fully connected layer input\n","    # Output shape: [batch_size, 4*4*16=256].\n","    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n","    # Fully connected layer, Output shape: [batch_size,120].\n","    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n","    # Apply ReLU non-linearity.\n","    fc1 = tf.nn.relu(fc1)\n","\n","    # Fully connected layer, Output shape: [batch_size, 84].\n","    fc2 = tf.add(tf.matmul(fc1, weights['wd2']), biases['bd2'])\n","    # Apply ReLU non-linearity.\n","    fc2 = tf.nn.relu(fc2)\n","\n","    # Fully connected layer, Output shape: [batch_size, 10].\n","    out = tf.add(tf.matmul(fc2, weights['w_out']), biases['bout'])\n","    # Apply softmax to generate a probability distribution.\n","    return tf.nn.softmax(out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWyqpTkcpvNZ"},"source":["# Cross-Entropy loss function.\n","def cross_entropy(y_pred, y_true):\n","    y_true = tf.one_hot(y_true, depth=num_classes)\n","    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n","    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n","    \n","# Accuracy metric.\n","def accuracy(y_pred, y_true):\n","    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n","    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n","    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n","\n","# ADAM optimizer.\n","optimizer = tf.optimizers.Adam(learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VOVDcsPMuJ39"},"source":["## Optimization process.\n","## Question 2:  Add L2 weight decay regularization\n","## Question 3:  Add L1 weight decay regularization"]},{"cell_type":"code","metadata":{"id":"jPA4UXzrpvNZ"},"source":["# Optimization process. \n","# reg_type:  1: L1 norm;  2: L2 norm;  other: Baseline without regularization\n","# lamda: regularizaton strength,  default is 0, without regularization. \n","def run_optimization(x, y, reg_type = 0, lamda = 0 ):\n","    # Wrap computation inside a GradientTape for automatic differentiation.\n","    with tf.GradientTape() as g:\n","        pred = conv_net(x)\n","        loss = cross_entropy(pred, y)\n","        # Add L1 or L2 weight decay regularization to the loss\n","        # Using  kernel_regularizer\n","        if reg_type == 1:       # L1 norm\n","            kernel_weights = tf.reduce_sum(tf.abs(weights['wc1'])) + tf.reduce_sum(tf.abs(weights['wc2'])) + tf.reduce_sum(tf.abs(weights['wd1'])) \\\n","             + tf.reduce_sum(tf.abs(weights['wd2'])) + tf.reduce_sum(tf.abs(weights['w_out']))   \n","            loss += lamda * kernel_weights\n","        elif reg_type == 2:     # L2 norm\n","            kernel_weights = tf.reduce_sum(tf.square(weights['wc1'])) + tf.reduce_sum(tf.square(weights['wc2'])) \\\n","            + tf.reduce_sum(tf.square(weights['wd1'])) + tf.reduce_sum(tf.square(weights['wd2'])) \\\n","            + tf.reduce_sum(tf.square(weights['w_out'])) \n","            loss += lamda * kernel_weights\n","        else:                   # no regularization\n","            pass\n","\n","    # trainable variables.\n","    trainable_variables = list(weights.values()) + list(biases.values())\n","    # Compute gradients.\n","    gradients = g.gradient(loss, trainable_variables)\n","    # Update W and b following gradients.\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dTtNTjA4jHui"},"source":["## Calculate the number of model for Question 4.\n","## Modify the architecture to remove the fully-connected layers at the backend of the network. For example, replace with Global Average Pooling or an alternative. Report the change in the number of parameters for this model compared to previous."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SO4dHm51ea6E","executionInfo":{"status":"ok","timestamp":1605842696118,"user_tz":360,"elapsed":419,"user":{"displayName":"Hongyu Guo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgexSoQ9_QdGEClNISOan7UQbqY6oUUgE9thCgEug=s64","userId":"07582815621439998065"}},"outputId":"5fadef87-9625-4927-f83b-8f7439c72041"},"source":["# Calculate the number of model for Question 4.\n","def model_size(model_params):\n","  num_para = 0\n","  for i in range(len(model_params)):\n","    num_para += len(tf.reshape(model_params[i],-1))\n","\n","  return num_para\n","\n","\n","trainable_variables = list(weights.values()) + list(biases.values())\n","Baseline_Size = model_size(trainable_variables)\n","print(Baseline_Size )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["44426\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TUG536hho1LQ"},"source":["# Question 5: three runs of train / test "]},{"cell_type":"code","metadata":{"id":"Uxdao_NR0uG0"},"source":["## Question 5: three runs of train / test "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pdng5ykxvwtB"},"source":["## Training  and Testing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_0azpSrpvNZ","executionInfo":{"status":"ok","timestamp":1605848724601,"user_tz":360,"elapsed":23736,"user":{"displayName":"Hongyu Guo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgexSoQ9_QdGEClNISOan7UQbqY6oUUgE9thCgEug=s64","userId":"07582815621439998065"}},"outputId":"1f8a23f8-b88c-4b87-827c-1760f1e049fe"},"source":["## Question 5: three runs of train / test \n","## set super parameters and run three times, manually \n","regularizer = 1   # 0: no regularization; 1: L1; 2: L2 \n","weight_decay = 1e-3  # 1e-4, 1e-3\n","\n","# Weights initialization \n","weights, biases = weights_init() # clear weights and biases before new run of train/test\n","start_time = time.time()\n","with tf.device('/device:GPU:0'):\n","    for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n","        run_optimization(batch_x, batch_y, regularizer, weight_decay)\n","        if step % display_step == 0:\n","            pred = conv_net(batch_x)\n","            loss = cross_entropy(pred, batch_y)/batch_size\n","            acc = accuracy(pred, batch_y)\n","            print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n","\n","end_time = time.time()\n","print('Training loss: %f \\n' %loss,  'Training acc: %f' % acc)\n","print ('Time cost: ', end_time - start_time)\n","\n","# Test model on validation set.\n","pred = conv_net(x_test)\n","loss = cross_entropy(pred, y_test)/y_test.shape[0]\n","print('Test loss: %f' % loss)\n","print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["step: 234, loss: 0.415999, accuracy: 0.859375\n","step: 468, loss: 0.394258, accuracy: 0.828125\n","step: 702, loss: 0.488098, accuracy: 0.828125\n","step: 936, loss: 0.275866, accuracy: 0.937500\n","step: 1170, loss: 0.278102, accuracy: 0.906250\n","step: 1404, loss: 0.402531, accuracy: 0.843750\n","step: 1638, loss: 0.373588, accuracy: 0.843750\n","step: 1872, loss: 0.330263, accuracy: 0.898438\n","step: 2106, loss: 0.272336, accuracy: 0.898438\n","step: 2340, loss: 0.314002, accuracy: 0.875000\n","Training loss: 0.314002 \n"," Training acc: 0.875000\n","Time cost:  23.348323106765747\n","Test loss: 0.357108\n","Test Accuracy: 0.869300\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0okBf8BTDmuU"},"source":["def run3():\n","    # Weights initialization \n","    weights, biases = weights_init() # clear weights and biases before new run of train/test\n","    start_time = time.time()\n","    with tf.device('/device:GPU:0'):\n","        for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n","            run_optimization(batch_x, batch_y, regularizer, weight_decay)\n","            \n","                pred = conv_net(batch_x)\n","                loss = cross_entropy(pred, batch_y)/batch_size\n","                acc = accuracy(pred, batch_y)\n","                print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n","\n","    end_time = time.time()\n","    print('Training loss: %f \\n' %loss,  'Training acc: %f' % acc)\n","    print ('Time cost: ', end_time - start_time)\n","\n","    # Test model on validation set.\n","    pred = conv_net(x_test)\n","    loss = cross_entropy(pred, y_test)/y_test.shape[0]\n","    print('Test loss: %f' % loss)\n","    print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IGss2rKADyot","executionInfo":{"status":"ok","timestamp":1605849040553,"user_tz":360,"elapsed":23989,"user":{"displayName":"Hongyu Guo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgexSoQ9_QdGEClNISOan7UQbqY6oUUgE9thCgEug=s64","userId":"07582815621439998065"}},"outputId":"c7f40201-8fa7-4687-838d-7ba849dbc4c3"},"source":["## Question 5: three runs of train / test \n","## set super parameters and run three times, manually \n","regularizer = 1      # 0: no regularization; 1: L1; 2: L2 \n","weight_decay = 1e-3  # 1e-4, 1e-3\n","run3()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["step: 234, loss: 0.345885, accuracy: 0.882812\n","step: 468, loss: 0.245843, accuracy: 0.921875\n","step: 702, loss: 0.466272, accuracy: 0.820312\n","step: 936, loss: 0.293578, accuracy: 0.882812\n","step: 1170, loss: 0.220783, accuracy: 0.890625\n","step: 1404, loss: 0.308055, accuracy: 0.867188\n","step: 1638, loss: 0.269659, accuracy: 0.890625\n","step: 1872, loss: 0.304259, accuracy: 0.898438\n","step: 2106, loss: 0.278094, accuracy: 0.906250\n","step: 2340, loss: 0.349781, accuracy: 0.859375\n","Training loss: 0.349781 \n"," Training acc: 0.859375\n","Time cost:  23.416645050048828\n","Test loss: 0.327695\n","Test Accuracy: 0.880100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S1JBT65Q8cgh","executionInfo":{"status":"ok","timestamp":1605848796490,"user_tz":360,"elapsed":335,"user":{"displayName":"Hongyu Guo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgexSoQ9_QdGEClNISOan7UQbqY6oUUgE9thCgEug=s64","userId":"07582815621439998065"}},"outputId":"9eec617a-2d0a-445d-e6c4-f105e561e6e1"},"source":["# FC layer 1\n","Hoyer_layer1 = Hoyer_index(weights['wd1'])  \n","# FC layer 2\n","Hoyer_layer2 = Hoyer_index(weights['wd2'])\n","\n","print(float(Hoyer_layer1), float(Hoyer_layer2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(1277.594, shape=(), dtype=float32) 30720\n","tf.Tensor(400.74744, shape=(), dtype=float32) 10080\n","0.9969193339347839 0.9947248101234436\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0wDyMfG_Dol","executionInfo":{"status":"ok","timestamp":1605847763285,"user_tz":360,"elapsed":317,"user":{"displayName":"Hongyu Guo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgexSoQ9_QdGEClNISOan7UQbqY6oUUgE9thCgEug=s64","userId":"07582815621439998065"}},"outputId":"b1ac7355-f6ba-4efd-e60a-182bdbe907c8"},"source":["y_test.shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-ROOkAT-dPN","executionInfo":{"status":"ok","timestamp":1605847605288,"user_tz":360,"elapsed":295,"user":{"displayName":"Hongyu Guo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgexSoQ9_QdGEClNISOan7UQbqY6oUUgE9thCgEug=s64","userId":"07582815621439998065"}},"outputId":"feba44aa-7cd3-42e8-c20b-28aa28aa0a59"},"source":["# Test model on validation set.\n","loss = 0 \n","pred = 0\n","pred = conv_net(x_test)\n","loss = cross_entropy(pred, y_test)\n","print('Test loss: %f' % loss)\n","print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test loss: 3414.680664\n","Test Accuracy: 0.876500\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IZCwfBnRJmNW"},"source":["## Question 4: Remove fully-connected layers"]},{"cell_type":"code","metadata":{"id":"XuQLnCl8Jb-Z"},"source":["# Q4: Remove fully-connected layers\n","def globalAveragePooling2d(x):\n","    return  tf.reduce_mean(x, axis=[1,2])\n","\n","# Weights initialization for Question 4. \n","def weights_init_Q4():\n","random_normal = tf.initializers.RandomNormal():\n","    weights = {\n","        # Conv Layer 1: 5x5 conv,  6 filters \n","        'wc1': tf.Variable(random_normal([5, 5, 1, conv1_filters])),\n","        # Conv Layer 2: 5x5 conv, 16 filters.\n","        'wc2': tf.Variable(random_normal([5, 5, conv1_filters, conv2_filters])),\n","        # FC Out Layer: 84 inputs, 10 units (total number of classes)\n","        'w_out': tf.Variable(random_normal([conv2_filters, num_classes]))\n","    }\n","    biases = {\n","        'bc1': tf.Variable(tf.zeros([conv1_filters])),\n","        'bc2': tf.Variable(tf.zeros([conv2_filters])),\n","        'bout': tf.Variable(tf.zeros([num_classes]))\n","    }\n","# Define model\n","def conv_net_Q4(x):\n","    # Input shape: [batch_size, 28, 28, 1]. \n","    # A batch of 28x28x1 (grayscale) images.\n","    x = tf.reshape(x, [-1, 28, 28, 1])\n","    # Convolution Layer. Output shape: [batch_size, 24, 24, 6].\n","    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n","    # Max Pooling. Output shape: [batch_size, 12, 12, 6].\n","    conv1 = maxpool2d(conv1, k=2)\n","   \n","    # Convolution Layer. Output shape: [batch_size, 8, 8, 16].\n","    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n","    # Max Pooling. Output shape: [batch_size, 4, 4, 16].\n","    conv2 = maxpool2d(conv2, k=2)\n","\n","    # Global Average Pooling. Output shape: [batch_size, 16].\n","    GAP1 = globalAveragePooling2d(conv2)\n","    # Fully connected layer, Output shape: [batch_size, 10].\n","    out = tf.add(tf.matmul(GAP1, weights['w_out']), biases['bout'])\n","    # Apply softmax to generate a probability distribution.\n","    return tf.nn.softmax(out)\n","\n","def run_optimization_Q4(x, y ):\n","    with tf.GradientTape() as g:\n","        pred = conv_net_Q4(x)\n","        loss = cross_entropy(pred, y)\n","    \n","    trainable_variables = list(weights.values()) + list(biases.values())\n","    gradients = g.gradient(loss, trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t9-v6vaGbKJN"},"source":["## Q4: Training  and Testing"]},{"cell_type":"code","metadata":{"id":"t5JWiNObWRI0"},"source":["# Q4: Training and Testing\n","# Weights initialization \n","weights, biases = weights_init_Q4() # clear weights and biases before new run of train/test\n","\n","# train model\n","start_time = time.time()\n","with tf.device('/device:GPU:0'):\n","    for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n","        run_optimization_Q4(batch_x, batch_y)\n","        if step % display_step == 0:\n","            pred = conv_net_Q4(batch_x)\n","            loss = cross_entropy(pred, batch_y)\n","            acc = accuracy(pred, batch_y)\n","            print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))\n","\n","end_time = time.time()\n","print ( 'Time cost: ', end_time - start_time)\n","\n","# Test model on validation set.\n","pred = conv_net_Q4(x_test)\n","print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yraz-PQnkbR-"},"source":["## Question 4: \n","## The change in the number of parameters for the model of Question 4 compared to previous is:\n","## 41684."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2DH-IJf3cV5h","executionInfo":{"status":"ok","timestamp":1605840758788,"user_tz":360,"elapsed":315,"user":{"displayName":"Hongyu Guo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgexSoQ9_QdGEClNISOan7UQbqY6oUUgE9thCgEug=s64","userId":"07582815621439998065"}},"outputId":"6caa38ae-3d5a-4480-d38d-23ab97ef2f9c"},"source":["trainable_variables = list(weights.values()) + list(biases.values())\n","Model_Q4_Size  = model_size(trainable_variables)\n","print('The difference of number of parameters between Baseline model and the model using GAP: \\n', \\\n","      Baseline_Size - Model_Q4_Size)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The difference of number of parameters between Baseline model and the model using GAP: \n"," 41684\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lAZkErQUxkQE"},"source":["## Question 6: Analyze the weights of the regularized models\n","## Hoyer's index "]},{"cell_type":"code","metadata":{"id":"3oUzBXl2pvNZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605852053662,"user_tz":360,"elapsed":248,"user":{"displayName":"Hongyu Guo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgexSoQ9_QdGEClNISOan7UQbqY6oUUgE9thCgEug=s64","userId":"07582815621439998065"}},"outputId":"77dff4a8-58ca-4e98-9c73-ab60c7def841"},"source":["\n","def Hoyer_index(w_fc):\n","    sum_cj = tf.reduce_sum(tf.abs(w_fc)) \n","    sqrt_sumsquare_cj = tf.sqrt(tf.reduce_sum(tf.square(w_fc))) \n","    # Number of elements of FC layer\n","    N = int(tf.size(w_fc)) \n","    # Hoyer's index\n","    return ((np.sqrt(N) - sum_cj/ sqrt_sumsquare_cj)*(1/(np.sqrt(N)-1)))\n","    \n","# FC layer 1\n","Hoyer_layer1 = Hoyer_index(weights['wd1'])  \n","# FC layer 2\n","Hoyer_layer2 = Hoyer_index(weights['wd2'])\n","\n","print(float(Hoyer_layer1), float(Hoyer_layer2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.46089068055152893 0.47526222467422485\n"],"name":"stdout"}]}]}